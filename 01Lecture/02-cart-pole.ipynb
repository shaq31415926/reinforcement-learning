{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be624c29-267a-48d8-a8cc-45e498df503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0719a4-8073-4dd0-b538-71b657187ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e136b9-6521-438e-b5b5-62d59f988a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b93f50-0934-4978-b609-f631f8f6cf12",
   "metadata": {},
   "source": [
    "# Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936c2b5e-2860-41a4-b266-b77ca2cd30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4366f5-cddf-4ed8-933c-18918afd8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc4d39b-1e9c-41ec-b2ae-70ba2d1e26e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.04511656,  0.00330043,  0.01300843,  0.00355254], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322a2e6-19b0-4dff-bf68-a78a4cfd03c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral state\n",
    "\n",
    "#env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdc9d8b-a731-46ad-b3c2-c422ed5b32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c2144c-c4b0-40fb-802e-7493d484361d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afb2f18a-3ab9-45c9-ab01-9b8da58e9c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67c2d35-bf61-45f4-a5bc-7f57ce42ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #: 1\n",
      "Score: 20.0\n",
      "Episode #: 2\n",
      "Score: 40.0\n",
      "Episode #: 3\n",
      "Score: 23.0\n",
      "Episode #: 4\n",
      "Score: 20.0\n",
      "Episode #: 5\n",
      "Score: 16.0\n",
      "Episode #: 6\n",
      "Score: 17.0\n",
      "Episode #: 7\n",
      "Score: 16.0\n",
      "Episode #: 8\n",
      "Score: 19.0\n",
      "Episode #: 9\n",
      "Score: 16.0\n",
      "Episode #: 10\n",
      "Score: 20.0\n",
      "Episode #: 11\n",
      "Score: 24.0\n",
      "Episode #: 12\n",
      "Score: 24.0\n",
      "Episode #: 13\n",
      "Score: 18.0\n",
      "Episode #: 14\n",
      "Score: 12.0\n",
      "Episode #: 15\n",
      "Score: 18.0\n",
      "Episode #: 16\n",
      "Score: 9.0\n",
      "Episode #: 17\n",
      "Score: 21.0\n",
      "Episode #: 18\n",
      "Score: 14.0\n",
      "Episode #: 19\n",
      "Score: 32.0\n",
      "Episode #: 20\n",
      "Score: 11.0\n",
      "Episode #: 21\n",
      "Score: 12.0\n",
      "Episode #: 22\n",
      "Score: 29.0\n",
      "Episode #: 23\n",
      "Score: 19.0\n",
      "Episode #: 24\n",
      "Score: 16.0\n",
      "Episode #: 25\n",
      "Score: 24.0\n",
      "Episode #: 26\n",
      "Score: 14.0\n",
      "Episode #: 27\n",
      "Score: 17.0\n",
      "Episode #: 28\n",
      "Score: 15.0\n",
      "Episode #: 29\n",
      "Score: 49.0\n",
      "Episode #: 30\n",
      "Score: 36.0\n",
      "Episode #: 31\n",
      "Score: 20.0\n",
      "Episode #: 32\n",
      "Score: 13.0\n",
      "Episode #: 33\n",
      "Score: 39.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;66;03m# we are taking random actions, left and right\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     n_state, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     14\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode #: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rl/lib/python3.11/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:187\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    184\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rl/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:298\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    297\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    299\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this shows the cartpole in the environment when we take random actions\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "episodes = 50\n",
    "\n",
    "for e in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() # we are taking random actions, left and right\n",
    "        n_state, reward, done, _, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode #: {e}\")\n",
    "    print(f\"Score: {score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717e5f3-13eb-4297-9d8d-17f6cd619dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750e57c-c8f3-4b5c-8b02-2bef31f4a818",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61540dc-ea5d-4333-b8e1-483ab25e638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install 'stable-baselines3[extra]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fdd18e6-f9d7-496b-9677-9fc6ec66f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO # the algorithm we are importing\n",
    "# reference: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# vectorise environment - train on multiple agents\n",
    "# wrapper around environment\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8d70984-6090-4da8-8c57-92313640f621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpolicy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActorCriticPolicy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VecEnv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgae_lambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclip_range\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclip_range_vf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize_advantage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ment_coef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvf_coef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_sde\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msde_sample_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrollout_buffer_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRolloutBuffer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrollout_buffer_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_kl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstats_window_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensorboard_log\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpolicy_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Proximal Policy Optimization algorithm (PPO) (clip version)\n",
       "\n",
       "Paper: https://arxiv.org/abs/1707.06347\n",
       "Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
       "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
       "Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
       "\n",
       "Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
       "\n",
       ":param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
       ":param env: The environment to learn from (if registered in Gym, can be str)\n",
       ":param learning_rate: The learning rate, it can be a function\n",
       "    of the current progress remaining (from 1 to 0)\n",
       ":param n_steps: The number of steps to run for each environment per update\n",
       "    (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
       "    NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
       "    See https://github.com/pytorch/pytorch/issues/29372\n",
       ":param batch_size: Minibatch size\n",
       ":param n_epochs: Number of epoch when optimizing the surrogate loss\n",
       ":param gamma: Discount factor\n",
       ":param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
       ":param clip_range: Clipping parameter, it can be a function of the current progress\n",
       "    remaining (from 1 to 0).\n",
       ":param clip_range_vf: Clipping parameter for the value function,\n",
       "    it can be a function of the current progress remaining (from 1 to 0).\n",
       "    This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
       "    no clipping will be done on the value function.\n",
       "    IMPORTANT: this clipping depends on the reward scaling.\n",
       ":param normalize_advantage: Whether to normalize or not the advantage\n",
       ":param ent_coef: Entropy coefficient for the loss calculation\n",
       ":param vf_coef: Value function coefficient for the loss calculation\n",
       ":param max_grad_norm: The maximum value for the gradient clipping\n",
       ":param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
       "    instead of action noise exploration (default: False)\n",
       ":param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
       "    Default: -1 (only sample at the beginning of the rollout)\n",
       ":param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
       ":param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation\n",
       ":param target_kl: Limit the KL divergence between updates,\n",
       "    because the clipping is not enough to prevent large update\n",
       "    see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
       "    By default, there is no limit on the kl div.\n",
       ":param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
       "    the reported success rate, mean episode length, and mean reward over\n",
       ":param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
       ":param policy_kwargs: additional arguments to be passed to the policy on creation\n",
       ":param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
       "    debug messages\n",
       ":param seed: Seed for the pseudo random generators\n",
       ":param device: Device (cpu, cuda, ...) on which the code should be run.\n",
       "    Setting it to auto, the code will be run on the GPU if possible.\n",
       ":param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/miniconda3/envs/rl/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PPO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4896ef7d-8ecb-43da-9f61-fd8507d04868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# create our environment again\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "# vectorise our environment\n",
    "env = DummyVecEnv([lambda:env])\n",
    "# import the algorithm\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=\"training/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9bed36f-4028-4d70-ab9e-edb2c6398d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to training/logs/PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 46   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 44   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009046177 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.0031      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.09        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 45.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010487996 |\n",
      "|    clip_fraction        | 0.082       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 32.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009480332 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 55.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005360077 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 54          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007947822 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010961464 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.9        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 58.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 348         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003811696 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.7        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00801    |\n",
      "|    value_loss           | 59.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 391         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006568589 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.74        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00739    |\n",
      "|    value_loss           | 23.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 47           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 434          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028423215 |\n",
      "|    clip_fraction        | 0.00537      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.553       |\n",
      "|    explained_variance   | 0.747        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.941        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00254     |\n",
      "|    value_loss           | 26.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1690a4b50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d08590d3-92e6-439e-90b6-f4797b142cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"training/models/cartpole_20000\"\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd604cc4-d572-4f80-965b-880d3c82ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51675f1-7e17-4186-9bbe-55dd051ee203",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97b5a9cc-8d59-4b12-8c99-c40b6d373e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "model_path = \"training/models/cartpole_20000\"\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6d5c109-4b05-4dbf-87eb-88c7cb62175f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6107ef8d-67f3-4dd6-8ac9-87cddc8d196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c138f10e-10c5-4e1f-a81a-f79a526d7dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5d4ab-bae3-4e17-ac86-939c2b52a770",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a0d0ad3-705d-49e5-a38e-89a7ab920fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Episode #: 1\n",
      "Score: 525.0\n",
      "Episode #: 2\n",
      "Score: 911.0\n",
      "Episode #: 3\n",
      "Score: 1103.0\n",
      "Episode #: 4\n",
      "Score: 511.0\n",
      "Episode #: 5\n",
      "Score: 453.0\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "model_path = \"training/models/cartpole_20000\"\n",
    "model = PPO.load(model_path, env=env)\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for e in range(1, episodes+1):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode #: {e}\")\n",
    "    print(f\"Score: {score}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d57c57-6a38-4b03-a6b6-ec324016ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (1.64.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (5.27.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dfdee8-81b0-4ab4-88ea-0df5f0cab9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Using cached h5py-3.11.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (4.11.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Using cached keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Using cached optree-0.11.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (45 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/envs/rl/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-macosx_10_15_x86_64.whl (259.6 MB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.11.0-cp311-cp311-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_10_14_x86_64.whl (2.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl (37 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.11.0-cp311-cp311-macosx_10_9_x86_64.whl (296 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, protobuf, optree, opt-einsum, ml-dtypes, h5py, google-pasta, gast, astunparse, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.27.0\n",
      "    Uninstalling protobuf-5.27.0:\n",
      "      Successfully uninstalled protobuf-5.27.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf372fd-7040-4102-8ab5-c7e7ba50563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'training/logs/PPO_8'\n",
    "!tensorboard --logdir={log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fed8da-e2a6-4ac2-9d5f-69d24d9e6a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
